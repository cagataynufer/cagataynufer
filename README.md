## Cagatay N√ºfer
üìç Munich, Germany | ‚úâÔ∏è cagataynufer@gmail.com | [GitHub](https://github.com/cagataynufer)

---

### ‚¨¢ SPECIALIZATION
- **Deep Learning for Tabular & Synthetic Data** (CTGAN, GANs, Tabular AutoML)
- **NLP** 

üí° **Main Interests:** Generative Models, Natural Language Processing, AI in Healthcare & Biotech

---

### ‚¨¢ CURRENT PROJECTS
**(Private projects in DL for NLP, available upon request due to academic integrity)**

**Deep Learning for NLP Assignments** (LMU, DL for NLP Course)  
**Hands-on implementations of NLP models.** These projects cover:

| **Repo** | **Description** | **Tech Stack** |
|---|---|---|
| ‚ú¶ `dl-for-nlp-BERT` | Fine-tuning BERT for **classification & summarization** tasks, evaluating attention mechanisms and tokenization impact | `PyTorch` `Transformers` `Hugging Face` `scikit-learn` `TensorBoard` `Plotly` |
| ‚ú¶ `dl-for-nlp-Attention_Mechanisms` | **Implementing & analyzing attention models** (Bahdanau, Luong) within **sequence-to-sequence NLP tasks**, examining attention weights and training dynamics | `TensorFlow` `Keras` `PyTorch` `Matplotlib` `TensorBoard` `Plotly` |
| ‚ú¶ `dl-for-nlp-transformer` | **Custom Transformer implementation from scratch**, exploring **self-attention, positional encoding**, and efficient forward passes | `PyTorch` `Positional Encoding` `NumPy` `Matplotlib` `TensorBoard` `Plotly` |
| ‚ú¶ `dl-for-nlp-RNN` | Building, tuning, and evaluating **RNN models (LSTM, GRU)** for text classification, exploring generalization across datasets | `PyTorch` `scikit-learn` `LSTMs` `GRUs` `Keras` `TensorBoard` `Plotly` |

---

### ‚¨¢ WORK & EDUCATION
üéì **LMU Munich - M.Sc. in Statistics & Data Science** *(Machine Learning specialization)*  
üéì **Bogazici University - B.Sc. in Industrial Engineering**

---

### ‚¨¢ CONNECT
üì© **Contact me:** cagataynufer@gmail.com  
